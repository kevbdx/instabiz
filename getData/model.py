# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VLye9AqHe8wlpO42UNvz09wAoBaMNPCL
"""

import json
import pandas as pd
import io
from pandas.io.json import json_normalize
from datetime import datetime

df = pd.read_json("data.json")
df.head(2)
df_postDetails = df
del df_postDetails['emojis']
del df_postDetails['user']
df_postDetails['img_description'] = df_postDetails['img_description'].astype(str).str[19:]
df_postDetails['hashtags'] = df_postDetails['hashtags'].apply(','.join)
del df_postDetails['hashtags']
del df_postDetails['comments_disabled']
df_postDetails['time'] = pd.to_datetime(df_postDetails['time'].astype(int), unit='s')
df_postDetails.head()
df_postDetails['followers'] = (df_postDetails['followers'].replace(r'[kmb]+$', '', regex=True).astype(float) * df_postDetails['followers'].str.extract(r'[\d\.]+([kmb]+)', expand=False).fillna(1).replace(['k','m', 'b'], [10**3, 10**6, 10**9]).astype(int))

#Stem - Lem - Tokenisation et POS 
import nltk # importation de la lib
from nltk.corpus import stopwords
nltk.download('stopwords') # téléchargement des mots
stop = stopwords.words("english")
from nltk.stem.porter import *
ps = PorterStemmer() #stemmatiseur
from nltk.stem.wordnet import WordNetLemmatizer
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
lmtzr = WordNetLemmatizer() #lemmatiseur
df_postDetails['description'] = df_postDetails['description'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
df_postDetails['description'] = df_postDetails['description'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split()]))
df_postDetails['description'] = df_postDetails['description'].apply(lambda x: ' '.join([lmtzr.lemmatize(word, 'v') for word in x.split()]))
df_postDetails['POS'] = df_postDetails['description'].apply(lambda x: nltk.pos_tag(nltk.word_tokenize(x)))

del df_postDetails['description']
del df_postDetails['POS']
df_postDetails.head(5)

# format data for trainning 
from sklearn.model_selection import train_test_split
#df_postDetails = pd.get_dummies(df_postDetails)
X = df_postDetails.get(['comments','followers','is_video']).values #img_description j'ai enlevé 
Y = df_postDetails["likes"].values
#X = pd.get_dummies('img_description')
features_train, features_test, target_train, target_test = train_test_split(
     X, Y, test_size=0.20, random_state=0)
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators = 100,max_depth=12,max_features='log2')
X.tolist()

#fit sur le train
model.fit(features_train,target_train)

#predict test
y_pred = model.predict(features_test)
y_pred

#get score : 
from sklearn.metrics import f1_score
#print(f1_score(predict, y_test, average='weighted'))
print('Score: ', model.score(X, Y))




